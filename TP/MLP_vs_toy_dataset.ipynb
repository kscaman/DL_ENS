{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjoxD0r5JiJntrau551wTK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kscaman/DL_ENS/blob/main/TP/MLP_vs_toy_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and visualizing MLPs on toy datasets\n",
        "\n",
        "In this practical, we are going to learn how to train a simple feed forward neural network (aka Multi-Layer Perceptron or MLP) on a synthetic dataset and visualize the result."
      ],
      "metadata": {
        "id": "LzxQ13chKKnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First import the three libraries we will use: **torch**, **numpy** (as np) and **matplotlib.pyplot** (as plt)."
      ],
      "metadata": {
        "id": "krX3ndDIKojY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4KtyY-lJVCP"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data generation\n",
        "\n",
        "Then, create a function `create_checkerboard` that returns two pytorch Tensors: `X` of shape $(100,2)$ where $100$ is the number of data points and each point is drawn uniformly at random in $[-2,2]^2$, and `y` of shape $(100,2)$ where $y_i=(1,0)$ if $X_{i,1} \\cdot X_{i,2} > 0$, and $y_i=(0,1)$ otherwise. The vectors $y_i$ are called **one-hot** encodings of the classes 0 and 1.\n",
        "\n",
        "**IMPORTANT:** Always test the shape of your tensor with `print(X.shape)` to verify that your are computing the right quantity.\n",
        "\n"
      ],
      "metadata": {
        "id": "tj5UoUBrK-Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Visualizing the dataset\n",
        "X, y = create_checkerboard()\n",
        "print(X.shape, y.shape)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y[:,0])\n",
        "plt.xlabel(\"$X_1$\")\n",
        "plt.ylabel(\"$X_2$\")\n",
        "plt.title(\"Checkerboard dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hOoApLjcJXdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model creation\n",
        "\n",
        "First, let's create a two layer MLP with ReLU activations, 2 inputs, 2 outputs, and 10 internal neurons using `torch.nn.Sequential`, `torch.nn.Linear` and `torch.nn.ReLU` (see documentation).\n",
        "\n",
        "Verify that the model outputs a 2-dimensional vector for each input data point on the checkerboard dataset."
      ],
      "metadata": {
        "id": "v2C2TRFST99w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "G453zpDYUuF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training pipeline\n",
        "\n",
        "The following code is a routine to visualize the output of the network and loss during training. Note that `epoch` is the number of epochs of training, data is a tuple containing `X, y`, `model` is a neural network and `losses` is a list of loss values."
      ],
      "metadata": {
        "id": "fD1ycoGwOgI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def visualize(epoch, data, model, losses):\n",
        "    X, y = data\n",
        "    # Creates a 2d grid\n",
        "    xx, yy = np.meshgrid(np.linspace(-2.5, 2.5, 100), np.linspace(-2.5, 2.5, 100))\n",
        "\n",
        "    # Uses the model to predict the class\n",
        "    grid_tensor = torch.FloatTensor(np.stack([xx.ravel(), yy.ravel()], axis=1))\n",
        "    with torch.no_grad():\n",
        "        Z = model(grid_tensor)\n",
        "        Z = Z[:,0] - Z[:,1]\n",
        "\n",
        "    # Reshapes the predictions to fit the grid\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plots the output of the neural network\n",
        "    fig = plt.figure(1, figsize=(10, 4))\n",
        "    plt.subplot(1,2,1)\n",
        "    CS = plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "    fig.colorbar(CS)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y[:,0])\n",
        "    plt.xlim([-2,2])\n",
        "    plt.ylim([-2,2])\n",
        "    plt.xlabel(\"$X_1$\")\n",
        "    plt.ylabel(\"$X_2$\")\n",
        "    plt.title(f\"Classification with an MLP (epoch = {epoch})\")\n",
        "\n",
        "    # Plots the loss function\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(losses)\n",
        "    plt.ylim([0,1])\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss vs. number of epochs\")\n",
        "    plt.tight_layout()\n",
        "    clear_output()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5PF30ZW9JpZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our objective is to push the model, for an input `X` to return an output that matches `y`. To do so we will minimize the **mean square error** $MSE = \\frac{1}{N}\\sum_{i=1}^N \\|\\mbox{model}(X_i) - y_i\\|^2$ over the entire dataset using gradient descent.\n",
        "\n",
        "Create a function `train(data, model, epochs)` that takes a dataset of inputs and outputs `X, y`, a neural network `model` and a number of `epochs`, and trains the neural network on the dataset for the specified number of epochs. You will need to create a loop over all epochs, in which you will:\n",
        "\n",
        "1. Prepare gradient computations using `model.zero_grad()`\n",
        "2. Compute the output of the `model` over the input data `X`\n",
        "3. Compute the mean square error loss over the dataset.\n",
        "4. Use `loss.backward()` to automatically compute the gradients of the loss wrt the model parameters.\n",
        "5. Make one step of gradient descent by updating each parameter `p` in `model.parameters()` with the formula $p = p - \\eta \\nabla L(p)$ where $\\eta=10^{-2}$ is the step-size and $L$ is the loss. Note that the gradient computed by `loss.backward()` is accessible in `p.grad`.\n",
        "6. Once every 1000 epochs, visualize the output of the neural network and loss.\n"
      ],
      "metadata": {
        "id": "i5wuXOEIP6A_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "LUWp4-aEJll_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, train your MLP on the checkerboard dataset for 10000 epochs. Is it predicting the correct classes? Try with 2, 10, and 100 neurons in the internal layer."
      ],
      "metadata": {
        "id": "MBfCk-QOVy8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "7EZV201dPrQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Harder dataset\n",
        "\n",
        "The following code create a more challenging dataset consisting of two intertwined spirals."
      ],
      "metadata": {
        "id": "zpYkb5fgWCup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_spiral():\n",
        "    # Créez une spirale\n",
        "    theta = np.linspace(0, 4 * np.pi, 100)\n",
        "    r = np.linspace(0.5, 2, 100)\n",
        "    x = r * np.cos(theta)\n",
        "    y = r * np.sin(theta)\n",
        "\n",
        "    # Créez une autre spirale avec une classe différente\n",
        "    theta = np.linspace(0, 4 * np.pi, 100)\n",
        "    r = np.linspace(0.5, 2, 100) + 0.3\n",
        "    x2 = r * np.cos(theta)\n",
        "    y2 = r * np.sin(theta)\n",
        "\n",
        "    # Fusionnez les deux ensembles de données\n",
        "    X = np.vstack((np.hstack((x, x2)), np.hstack((y, y2)))).T\n",
        "    y = np.hstack((np.zeros(100), np.ones(100)))\n",
        "\n",
        "    # De Numpy arrays à PyTorch Tensors\n",
        "    X = torch.FloatTensor(X)\n",
        "    y = torch.LongTensor([y,1-y]).T\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Visualisez les données\n",
        "X, y = create_spiral()\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y[:,0])\n",
        "plt.xlabel(\"$X_1$\")\n",
        "plt.ylabel(\"$X_1$\")\n",
        "plt.title(\"Spiral dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ojjFvkCjQJst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train your model (craete a new model to reinitialize the parameters) on the spiral dataset for 50000 epochs. Is it predicting the correct classes? Try increasing the number of neurons."
      ],
      "metadata": {
        "id": "rDX8nmPhWN3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "_l--6uvUVtoK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}