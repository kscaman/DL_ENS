{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kscaman/DL_ENS/blob/main/DL_ENS_Optimization_algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5yGqm4Nl39f"
      },
      "source": [
        "# Optimization algorithms\n",
        "\n",
        "Code adapted from [DataFlowr](https://dataflowr.github.io/website/modules/4-optimization-for-deep-learning/), itself adapted from the [optimization chapter](http://www.d2l.ai/chapter_optimization/) of Dive into Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gJMsVnYl39r"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def get_gradient(f, x, noise_std=0):\n",
        "    \"\"\"Returns the gradient of a function f at x with additive Gaussian noise.\"\"\"\n",
        "    x = x.detach() # Removes information about the gradient\n",
        "    x.requires_grad = True\n",
        "    output = f(x)\n",
        "    output.backward()\n",
        "    gradient = x.grad\n",
        "    noise = torch.randn(gradient.shape)\n",
        "    return gradient + noise_std * noise\n",
        "\n",
        "def optimize(update, num_iter=20, init_s=0):\n",
        "    \"\"\"Optimize the objective function of 2d variables with a customized update.\"\"\"\n",
        "    \"\"\"update(x,s_x) should return the updated positions x and possible memory terms.\"\"\"\n",
        "    x = torch.Tensor([-5, -2])\n",
        "    s = init_s\n",
        "    all_x = [x]\n",
        "    for i in range(num_iter):\n",
        "        x, s = update(x, s)\n",
        "        all_x.append((x))\n",
        "    print('epoch %d, x1 %f, x2 %f' % (i+1, x[0], x[1]))\n",
        "    return torch.stack(all_x, dim=0)\n",
        "\n",
        "def show_trace_2d(f, all_x, color='red', is_new_plot=True):\n",
        "    \"\"\"Show the trace of 2d variables during optimization.\"\"\"\n",
        "    # Plot the iterates x_t\n",
        "    if is_new_plot:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    else:\n",
        "        fig = plt.gcf()\n",
        "        ax1 = fig.axes[0]\n",
        "        ax2 = fig.axes[1]\n",
        "    ax1.plot(all_x[:,0], all_x[:,1], '-o', color=color)\n",
        "    all_f = np.array([f(torch.Tensor([x[0], x[1]])) for x in all_x])\n",
        "\n",
        "    # Plot the function's level sets\n",
        "    x1 = np.arange(-5.5, 3.5, 0.1)\n",
        "    x2 = np.arange(-3.0, 2.0, 0.1)\n",
        "    f_grid = np.array([[f(torch.Tensor([u, v])) for u in x1] for v in x2])\n",
        "    ax1.contour(x1, x2, f_grid, colors='blue')\n",
        "    ax1.set_xlim([-5.5, 3.5])\n",
        "    ax1.set_ylim([-3.0, 2.0])\n",
        "    ax1.set_xlabel('x1')\n",
        "    ax1.set_ylabel('x2')\n",
        "\n",
        "    # Plot the function values f(x_t)\n",
        "    ax2.plot(all_f, color=color)\n",
        "    ax2.set_xlabel('time ($t$)')\n",
        "    ax2.set_ylabel('function value ($f(x_t)$)')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The function to optimize\n",
        "def f(x):\n",
        "    return 0.1 * x[0] ** 2 + 2 * x[1] ** 2"
      ],
      "metadata": {
        "id": "Mp25f8OkqSwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlFPt-C9l3-N"
      },
      "source": [
        "## Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVnxAGegl3-R"
      },
      "outputs": [],
      "source": [
        "eta = 0.4\n",
        "\n",
        "def gd(x, s):\n",
        "    gradient = get_gradient(f, x)\n",
        "    x = x - eta * gradient\n",
        "    return x, 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npaYk5WUl3-e"
      },
      "outputs": [],
      "source": [
        "eta = 0.4\n",
        "show_trace_2d(f, optimize(gd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VfX0pq7l3-q"
      },
      "outputs": [],
      "source": [
        "eta = 0.6\n",
        "show_trace_2d(f, optimize(gd))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcsKwAyRl3-y"
      },
      "source": [
        "## Momemtum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qnyJQwnl3-0"
      },
      "outputs": [],
      "source": [
        "eta, gamma = 0.4, 0.5\n",
        "\n",
        "def momentum(x, v):\n",
        "    #\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    return x, v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT8vz7dKl3-7"
      },
      "outputs": [],
      "source": [
        "eta = 0.4\n",
        "show_trace_2d(f, optimize(momentum))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.6\n",
        "show_trace_2d(f, optimize(momentum))"
      ],
      "metadata": {
        "id": "Lhw5jlXHdoIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSZCrah6l3_K"
      },
      "outputs": [],
      "source": [
        "eta, gamma = 0.05, 0.9\n",
        "show_trace_2d(f, optimize(momentum))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WjZp2WPl3_T"
      },
      "source": [
        "## Nesterov accelerated gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s02JrcZRl3_V"
      },
      "outputs": [],
      "source": [
        "eta, gamma = 0.05, 0.9\n",
        "\n",
        "def nesterov(x, v):\n",
        "    #\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    return x, v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upxkjOqIl3_b"
      },
      "outputs": [],
      "source": [
        "show_trace_2d(f, optimize(nesterov))\n",
        "show_trace_2d(f, optimize(momentum), color= 'green')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyzF7aAXl3_h"
      },
      "source": [
        "## Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mk4sixgl3_i"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def adagrad(x, s):\n",
        "    eps = 1e-6\n",
        "    #\n",
        "    # YOUR CODE HERE (hint: use x**a for coordinate-wise power a)\n",
        "    #\n",
        "    return x, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVc4sPtFl3_o"
      },
      "outputs": [],
      "source": [
        "eta = 0.4\n",
        "show_trace_2d(f, optimize(adagrad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY90pzoLl3_v"
      },
      "outputs": [],
      "source": [
        "eta = 1.5\n",
        "show_trace_2d(f, optimize(adagrad))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtU9tLiml3_2"
      },
      "source": [
        "## RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBbRL5h3l3_3"
      },
      "outputs": [],
      "source": [
        "def rmsprop(x, s):\n",
        "    eps = 1e-6\n",
        "    #\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    return x, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc3_6I-Ul3_9"
      },
      "outputs": [],
      "source": [
        "eta, gamma = 0.4, 0.9\n",
        "show_trace_2d(f, optimize(rmsprop))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD9ILdGVl4AD"
      },
      "source": [
        "## Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC3McRDZl4AF"
      },
      "outputs": [],
      "source": [
        "def adam(x, s):\n",
        "    beta1, beta2, eps = 0.9, 0.99, 1e-6\n",
        "    m, v, t = s\n",
        "    #\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    return x, (m, v, t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeQbFML-l4AV"
      },
      "outputs": [],
      "source": [
        "eta = 0.8\n",
        "show_trace_2d(f, optimize(adam, init_s=(0, 0, 0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLmMd9h4l4Aa"
      },
      "source": [
        "## AMSGrad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khhsvs3Dl4Ac"
      },
      "outputs": [],
      "source": [
        "def amsgrad(x, s):\n",
        "    beta1, beta2, eps = 0.9, 0.99, 1e-6\n",
        "    m, v, v_bar = s\n",
        "    #\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    return x, (m, v, v_bar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmJEncF-l4Ag"
      },
      "outputs": [],
      "source": [
        "eta = 1\n",
        "show_trace_2d(f, optimize(amsgrad, init_s=(0, 0, torch.Tensor([0, 0]))))\n",
        "show_trace_2d(f, optimize(adam, init_s=(0, 0, 0)), color ='green', is_new_plot=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic optimization and schedulers\n",
        "\n",
        "We will now see what happens when the gradient is noisy."
      ],
      "metadata": {
        "id": "4egXGTwJKO1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.4\n",
        "\n",
        "def sgd(x, s):\n",
        "    gradient = get_gradient(f, x, noise_std=0.8)\n",
        "    x = x - eta * gradient\n",
        "    return x, 0"
      ],
      "metadata": {
        "id": "8fud-H4nKpmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.4\n",
        "show_trace_2d(f, optimize(sgd, num_iter=200))\n",
        "eta = 0.2\n",
        "show_trace_2d(f, optimize(sgd, num_iter=200), color='green', is_new_plot=False)\n",
        "eta = 0.05\n",
        "show_trace_2d(f, optimize(sgd, num_iter=200), color='blue', is_new_plot=False)"
      ],
      "metadata": {
        "id": "euBMai3DKapp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A high learning rate is faster, but a small learning rate leads to better final performance.\n",
        "Propose a modification of sgd that use behavior this to his advantage."
      ],
      "metadata": {
        "id": "ylIFUmv0pCDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd_with_scheduler(x, t):\n",
        "    #\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    return x, t"
      ],
      "metadata": {
        "id": "jKVlpBm0ovJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.4\n",
        "show_trace_2d(f, optimize(sgd, num_iter=200))\n",
        "eta = 0.2\n",
        "show_trace_2d(f, optimize(sgd, num_iter=200), color='green', is_new_plot=False)\n",
        "eta = 0.05\n",
        "show_trace_2d(f, optimize(sgd, num_iter=200), color='blue', is_new_plot=False)\n",
        "show_trace_2d(f, optimize(sgd_with_scheduler, num_iter=200), color='yellow', is_new_plot=False)"
      ],
      "metadata": {
        "id": "yqVqeE4tq7yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a dataset of two Gaussian classes and 100 samples\n",
        "N = 100\n",
        "v = torch.Tensor([1,1]).unsqueeze(0)\n",
        "Y = 2 * torch.randint(low=0, high=2, size=(N, 1), dtype=torch.float) - 1\n",
        "X = 1 * torch.randn(N, 2) + Y @ v\n",
        "plt.plot(X[Y.flatten() > 0,0], X[Y.flatten() > 0,1], 'o', color='red')\n",
        "plt.plot(X[Y.flatten() < 0,0], X[Y.flatten() < 0,1], 'o', color='blue')"
      ],
      "metadata": {
        "id": "fDjhHB70tupL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_linear(theta):\n",
        "    #\n",
        "    # YOUR CODE HERE\n",
        "    # WARNING: theta should be of size (2,1)\n",
        "    #\n",
        "\n",
        "mini_batch_size = 1\n",
        "def mse_linear_mb(theta, t):\n",
        "    #\n",
        "    # YOUR CODE HERE\n",
        "    # WARNING: theta should be of size (2,1)\n",
        "    # HINT: use torch.randperm\n",
        "    #"
      ],
      "metadata": {
        "id": "7mD11e7398P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(x, s):\n",
        "    beta1, beta2, eps = 0.9, 0.99, 1e-6\n",
        "    m, v, t = s\n",
        "    #\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    return x, (m, v, t)\n",
        "\n",
        "eta = 0.1\n",
        "mini_batch_size = 1\n",
        "show_trace_2d(mse_linear, optimize(adam, num_iter=100, init_s=(0, 0, 0)))\n",
        "mini_batch_size = 5\n",
        "show_trace_2d(mse_linear, optimize(adam, num_iter=100, init_s=(0, 0, 0)), color='green', is_new_plot=False)\n",
        "mini_batch_size = 100\n",
        "show_trace_2d(mse_linear, optimize(adam, num_iter=100, init_s=(0, 0, 0)), color='blue', is_new_plot=False)"
      ],
      "metadata": {
        "id": "Wm6m5pwqwVBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch optimizers and schedulers\n",
        "Typical training loop, optimizers and schedulers in Pytorch"
      ],
      "metadata": {
        "id": "K72-R5GBtIAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MINIMAL (AND INCOMPLETE) EXAMPLE\n",
        "for epoch in range(20):\n",
        "    for inputs, targets in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "BG9AtGl4YF1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MORE DETAILED AND COMPLETE EXAMPLE\n",
        "\n",
        "### PARAMETERS\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 100\n",
        "\n",
        "### DATA, MODEL, LOSS, OPTIMIZER AND SCHEDULER\n",
        "dataloader = ... # YOUR DATA\n",
        "model = ... # YOUR MODEL\n",
        "criterion = ... # YOUR LOSS FUNCTION\n",
        "optimizer = ... # YOUR OPTIMIZER\n",
        "scheduler = ... # YOUR SCHEDULER\n",
        "\n",
        "### TRAINING LOOP\n",
        "# Prepares the model for training (needed for some models)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # One training epoch over the whole dataset\n",
        "    for inputs, targets in dataloader:\n",
        "        # One mini-batch, put on the desired devide (cpu or gpu)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        # Reinitialize the gradients before any computation\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Computation of the model's output and loss on the mini-batch\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Computation of the gradient on the mini-batch\n",
        "        loss.backward()\n",
        "        \n",
        "        # One iteration of the optimizer and update of the step-size\n",
        "        optimizer.step()\n",
        "\n",
        "        # Then we can compute statistics and store loss values\n",
        "        ...\n",
        "        \n",
        "    # Update of the step-size\n",
        "    scheduler.step()\n",
        "    print('Loss: {:.4f} Acc: {:.4f}'.format(..., ...))"
      ],
      "metadata": {
        "id": "sDAB3a6utNtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Typical optimizers and schedulers\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: 1/t**0.5)"
      ],
      "metadata": {
        "id": "71m6hmkoTLfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to implement your own optimizer\n",
        "class SuperDuperOptimizer(optim.Optimizer):\n",
        "    def __init__(self, params, lr):\n",
        "        defaults = dict(lr=lr)\n",
        "        super(SuperDuperOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    def step():\n",
        "        ..."
      ],
      "metadata": {
        "id": "iv4ZaNF4X2Xa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "j5yGqm4Nl39f",
        "4egXGTwJKO1I",
        "K72-R5GBtIAO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
