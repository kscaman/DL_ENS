{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kscaman/DL_ENS/blob/main/TP/deep_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJiauBGr6hUG"
      },
      "source": [
        "# Deep Reinforcement Learning\n",
        "\n",
        "In this practical, our objective is to replicate the famous [Deepmind paper on DQN](https://deepmind.google/discover/blog/deep-reinforcement-learning/) and train, using deep Q learning, an agent to play the Atari game [Breakout](https://www.gymlibrary.dev/environments/atari/breakout/)!\n",
        "The code is adapted from the following [Github repo](https://github.com/jacobaustin123/pytorch-dqn/) and the [Pytorch tutorial on DQL](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSDfGk7DmOE5"
      },
      "outputs": [],
      "source": [
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]\n",
        "\n",
        "from IPython import display\n",
        "from IPython.display import HTML\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4ZQ9LYLgh1V"
      },
      "source": [
        "First load the environment using `env = gym.make('BreakoutDeterministic-v4')` (you can search the [gym documentation](https://www.gymlibrary.dev/environments/atari/breakout/)). The environment is initialized through `env.reset()`, updated via `env.step(action)`, and a random action can be created via `env.action_space.sample()`. Plot the state, action and reward after one random step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzApwg6-nbEw"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In what follows, we will encode a policy as a function `policy` that takes as input a state and returns a vector containing Q scores for each action (as a tensor of shape $(1,a)$ where $a$ is the number of possible actions). The following code will be used to create a video from a policy."
      ],
      "metadata": {
        "id": "wVPrs92prD3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_video(policy, num_frames=100, preprocess=None):\n",
        "    def animation_update(num):\n",
        "        progress_bar.update(1)\n",
        "        ax.clear()\n",
        "        state = env.render(\"rgb_array\")\n",
        "        ax.imshow(state)\n",
        "        state = state if preprocess is None else preprocess(state)\n",
        "        action = policy(state)\n",
        "        env.step(action)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 4), dpi=100)\n",
        "    env.reset()\n",
        "    for _ in range(np.random.randint(1, 10)):\n",
        "        env.step(0)\n",
        "    state, _, done, _ = env.step(env.action_space.sample())\n",
        "    progress_bar = tqdm(total=num_frames)\n",
        "    anim = animation.FuncAnimation(fig, animation_update, frames=num_frames, interval=50)\n",
        "    anim = HTML(anim.to_html5_video())\n",
        "    progress_bar.close()\n",
        "    plt.close()\n",
        "    return anim"
      ],
      "metadata": {
        "id": "_iZxGKnllCVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcm81lJAlEf-"
      },
      "source": [
        "Create a video of a random agent playing Breakout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttModBgQljJW"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djlaMApyoXwx"
      },
      "source": [
        "Create a class `EpsilonGreedy(policy, epsilon)` that takes a policy and returns its corresponding epsilon greedy strategy. To do so, create a method `__call__(self, state)` that returns a random action with probability epsilon, and the action chosen by the policy otherwise. Note that the output of the method should have a shape $(1,1)$.\n",
        "\n",
        "Finally, create a video of an epsilon-greedy strategy that goes left expect with probability $\\varepsilon = 0.5$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk03W1O0lH3X"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaAc3mhe-oFr"
      },
      "source": [
        "## State pre-processing\n",
        "We would like to simplify the learning task by pre-processing the frames in two ways: 1) image pre-processing and 2) using frame sequences.\n",
        "\n",
        "1) The atari frame is large and contains scores at the top of the screen that is not useful for our agent (as a reward is already available). Using `torchvision.transforms` methods (`ToPILImage`, `Grayscale`, `Resize`, `functional.crop`), crop the image to a $144\\times 144$ image in grayscale, and flatten the resulting image to a vector of size $144\\times 144$. Test this preprocessing on an Atari frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1G2bzfP-nmY"
      },
      "outputs": [],
      "source": [
        "img_preprocess = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToPILImage(),\n",
        "    torchvision.transforms.Grayscale(),\n",
        "    lambda x : torchvision.transforms.functional.crop(x, 25, 8, 180, 144),\n",
        "    torchvision.transforms.Resize((84, 84), 0),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To allow our model to access the speed and direction of the ball, augment the state of the environment by storing a sequence of 4 consecutive frames. To do so, create a class `FullPreprocessing` whose method `__call__(state)` updates a field `history` made of a concatenation of the 4 previous states and returns it."
      ],
      "metadata": {
        "id": "g3HBOTifoTpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "QzI_mIu6HJuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzy4BPxc8cJ5"
      },
      "source": [
        "## Deep Q Learning algorithm\n",
        "We are now going to implement the DQN algorithm. First, we need to encode the policy using a neural network. Create a convolutional neural network that takes a sequence of 4 frames from the game and returns a score for each possible action. The CNN will consist of 3 convlution layers `nn.Conv2d(4, 32, 8, stride=4), nn.Conv2d(32, 64, 4, stride=2), nn.Conv2d(64, 64, 3, stride=1)` followed by an MLP with 2 linear layers `nn.Linear(7 * 7 * 64, 1024), nn.Linear(1024, 4)` and `F.leaky_relu` as activation functions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "mRGzg1FhH-aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To speed up the training, let's start from a pre-trained model and learn only the last layer. Create a function `good_init(model, filename)` that loads the model available [here](https://github.com/jacobaustin123/pytorch-dqn/blob/master/weights/breakout/good.pt), fix its parameters (`p.requires_grad=False` on all parameters p), and reinitialize the last layer."
      ],
      "metadata": {
        "id": "F0dMdYYHCFCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "nGeL9U2kCD96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ0CjnNh4BFU"
      },
      "source": [
        "We then define a replay memory to sample past (state, action, reward) tuples."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Memory:\n",
        "    def __init__(self, max_size):\n",
        "        self.max_size = max_size\n",
        "        self.full = False\n",
        "        self.curr = 0\n",
        "\n",
        "        self.states = torch.zeros((self.max_size, 84, 84), dtype=torch.uint8, device=device)\n",
        "        self.rewards = torch.zeros((self.max_size, 1), device=device)\n",
        "        self.actions = torch.zeros((self.max_size, 1), dtype=torch.uint8, device=device)\n",
        "        self.terminals = torch.zeros((self.max_size, 1), dtype=torch.uint8, device=device)\n",
        "\n",
        "    def store(self, state, action, reward, terminal):\n",
        "        idx = self.curr % self.max_size\n",
        "\n",
        "        self.states[idx] = (state * 255).to(torch.uint8).to(device)\n",
        "        self.actions[idx] = action\n",
        "        self.rewards[idx] = reward\n",
        "        self.terminals[idx] = terminal\n",
        "\n",
        "        self.increment()\n",
        "\n",
        "    def size(self):\n",
        "        return self.max_size if self.full else self.curr\n",
        "\n",
        "    def increment(self):\n",
        "        if self.curr + 1 == self.max_size:\n",
        "            self.full = True\n",
        "\n",
        "        self.curr = (self.curr + 1) % self.max_size\n",
        "\n",
        "    def index(self, idx):\n",
        "        if len(idx) == 0 or self.size() < idx.max():\n",
        "            raise ValueError(\"Not enough elements in cache to sample {} elements\".format(len(idx)))\n",
        "\n",
        "        return (self.states[idx].to(device).to(torch.float32) / 255.), \\\n",
        "                self.actions[idx].to(device).to(torch.long), \\\n",
        "                self.rewards[idx].to(device), \\\n",
        "                self.terminals[idx].to(device).to(torch.int16)\n",
        "\n",
        "    def _process_idx(self, idx):\n",
        "        return idx % self.max_size\n",
        "\n",
        "    def sample(self, N):\n",
        "        if self.size() - 4 < N:\n",
        "            raise ValueError(\"Not enough elements in cache to sample {} elements\".format(N))\n",
        "\n",
        "        idx = np.random.choice(self.size() - 4, N)\n",
        "        idx = self._process_idx(idx)\n",
        "        state_idx = self._process_idx((idx.reshape(-1, 1) + np.array([0, 1, 2, 3]).reshape(1, -1)).flatten())\n",
        "        next_state_idx = self._process_idx((idx.reshape(-1, 1) + np.array([0, 1, 2, 3]).reshape(1, -1)).flatten() + 1)\n",
        "\n",
        "        states = []\n",
        "        next_states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        terminals = []\n",
        "\n",
        "        _, a, r, t = self.index(idx)\n",
        "        s, _, _, _ = self.index(state_idx)\n",
        "        ns, _, _, _ = self.index(next_state_idx)\n",
        "\n",
        "        states.append(s)\n",
        "        next_states.append(ns)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "        terminals.append(t)\n",
        "\n",
        "        return torch.cat(states).reshape(-1, 4, states[0].shape[1], states[0].shape[2]), \\\n",
        "               torch.cat(next_states).reshape(-1, 4, states[0].shape[1], states[0].shape[2]), \\\n",
        "               torch.cat(actions), \\\n",
        "               torch.cat(rewards), \\\n",
        "               torch.cat(terminals)\n"
      ],
      "metadata": {
        "id": "ssnONmioHnee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7j8QzyWFy9W"
      },
      "source": [
        "The optimization step performs one step of Q-Learning. Fill in the blanks by computing the loss function, computing the correct gradient, clipping the gradient, and performing one step of the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optim_step(mem, model, target_model, loss_func, optimizer):\n",
        "    states, next_states, actions, rewards, terminals = mem.sample(BATCH_SIZE)\n",
        "    mask = (1 - terminals).float()\n",
        "    y = rewards + mask * GAMMA * torch.max(target_model(next_states), dim=1).values.view(-1, 1).detach()\n",
        "    y = y.squeeze()\n",
        "    x = model(states)[range(BATCH_SIZE), actions.squeeze()]\n",
        "\n",
        "    ### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "jHwA2ugTFy9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code initializes the training parameters, models and optimizers."
      ],
      "metadata": {
        "id": "k4GX2O9rGAo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MEM_SIZE = int(1e5) # this is either 250k or 1 million in the paper (size of replay memory)\n",
        "EPISODES = int(2e3) # total training episodes\n",
        "BATCH_SIZE = 32 # minibatch update size\n",
        "GAMMA = 0.99 # discount factor\n",
        "UPDATE_FREQ = 4 # perform minibatch update once every UPDATE_FREQ\n",
        "TARGET_UPDATE_EVERY = 40000 # (frames)\n",
        "INIT_MEMORY_SIZE = 1000 #20000 # initial size of memory before minibatch updates begin\n",
        "TEST_EVERY = 100 # (episodes)\n",
        "NUM_TEST = 5\n",
        "\n",
        "mem = Memory(MEM_SIZE)\n",
        "model = Model().to(device)\n",
        "good_init(model)\n",
        "model = model.to(device)\n",
        "target_model = Model().to(device)\n",
        "target_model.load_state_dict(model.state_dict())\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5, eps=1.5e-4)\n",
        "loss_func = nn.SmoothL1Loss()\n",
        "num_frames = 0"
      ],
      "metadata": {
        "id": "KPabJT92Hw7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally the training loop! Fill in the gaps."
      ],
      "metadata": {
        "id": "FrppqeYuGLph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    print(\"[TESTING]\")\n",
        "    total_reward = 0\n",
        "    policy = EpsilonGreedy(model, 0.01)\n",
        "\n",
        "    for i in range(NUM_TEST):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "    total_reward /= NUM_TEST\n",
        "    print(f\"[TESTING] Total Reward: {total_reward:.1f}\")\n",
        "\n",
        "def train(episode, num_frames):\n",
        "    epsilon = max(0.1, 1 - (0.9 * num_frames) / 2e4)\n",
        "    policy = EpsilonGreedy(model, epsilon)\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    while not done:\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        if mem.size() < INIT_MEMORY_SIZE:\n",
        "            continue\n",
        "\n",
        "        if num_frames % UPDATE_FREQ == 0:\n",
        "            optim_step(mem, model, target_model, loss_func, optimizer)\n",
        "\n",
        "        if num_frames % TARGET_UPDATE_EVERY == 0: # reset target to source\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    print(f\"[EPISODE {episode}] Total Reward: {total_reward:.1f}, Epsilon: {epsilon:.2f}, Total Frames: {num_frames}, Memory Size: {mem.size()}\")\n",
        "    return num_frames\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    num_frames = train(episode, num_frames)\n",
        "    if episode % TEST_EVERY == 0:\n",
        "        test()"
      ],
      "metadata": {
        "id": "-Bf0zWNZMQrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkzV-N0471dY"
      },
      "source": [
        "Create a video of the new agent playing Breakout. How is the agent performing?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "0EpvWfjPcg68"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMh7EnwEPTOENRlyfiM1/6o",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}