{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEk5ik+ALyx/UgpRvJ0rWP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kscaman/DL_ENS/blob/main/TD/deep_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Reinforcement Learning\n",
        "\n",
        "In this practical, we are going to use Deep Q Learning to train an agent to play the Atari game [Breakout](https://www.gymlibrary.dev/environments/atari/breakout/).\n",
        "The code adapted from the [Pytorch tutorial on DQL](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html).\n"
      ],
      "metadata": {
        "id": "oJiauBGr6hUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSDfGk7DmOE5"
      },
      "outputs": [],
      "source": [
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "from IPython import display\n",
        "from IPython.display import HTML\n",
        "from itertools import count\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import models,transforms,datasets\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First load the environment using `gym.make(env_name)` (you can search the [gym documentation](https://www.gymlibrary.dev/environments/atari/breakout/))."
      ],
      "metadata": {
        "id": "f4ZQ9LYLgh1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = ### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "UzApwg6-nbEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The environment is initialized through `env.reset()`, and updated via `env.step(action)`. Plot the state, action and reward after one step."
      ],
      "metadata": {
        "id": "FHqxABzuhrwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "action = env.action_space.sample()\n",
        "state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ],
      "metadata": {
        "id": "yE2T6iG_5-bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_video(policy, num_frames=100, preprocess=None):\n",
        "    def animation_update(num):\n",
        "        progress_bar.update(1)\n",
        "        ax.clear()\n",
        "        state = env.render(\"rgb_array\")\n",
        "        ax.imshow(state)\n",
        "        preprocessed_state = state if preprocess is None else preprocess(state)\n",
        "        action = policy(preprocessed_state)\n",
        "        env.step(action)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 4), dpi=100)\n",
        "    env.reset()\n",
        "    progress_bar = tqdm(total=num_frames)\n",
        "    anim = animation.FuncAnimation(fig, animation_update, frames=num_frames, interval=50)\n",
        "    anim = HTML(anim.to_html5_video())\n",
        "    progress_bar.close()\n",
        "    plt.close()\n",
        "    return anim"
      ],
      "metadata": {
        "id": "7w82xfPZqVku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a video of a random agent playing Breakout."
      ],
      "metadata": {
        "id": "Bcm81lJAlEf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ],
      "metadata": {
        "id": "ttModBgQljJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an epsilon-greedy strategy that takes a policy encoded as a function that returns the Q function $q(s,a)$ for a given state $s$ as a vector, and return its corresponding epsilon greedy action. The output should have a shape $(1,1)$."
      ],
      "metadata": {
        "id": "djlaMApyoXwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedy:\n",
        "    def __init__(self, policy, epsilon):\n",
        "        self.policy = policy\n",
        "        self.epsilon = epsilon\n",
        "    \n",
        "    def __call__(self, state):\n",
        "        #\n",
        "        # YOUR CODE HERE\n",
        "        #"
      ],
      "metadata": {
        "id": "gk03W1O0lH3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a video of an epsilon-greedy strategy that goes left expect with probability $\\varepsilon = 0.5$."
      ],
      "metadata": {
        "id": "sgtBRuxOrpzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ],
      "metadata": {
        "id": "fYAwway3r4-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image preprocessing\n",
        "The atari frame is large and contains scores at the top of the screen that is not useful for our agent (as a reward is already available). Using `transforms`, crop the image to a $144\\times 144$ image in grayscale, and flatten the resulting image to a vector of size $144\\times 144$. Test this preprocessing on an Atari frame."
      ],
      "metadata": {
        "id": "MaAc3mhe-oFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    #\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "])\n",
        "\n",
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ],
      "metadata": {
        "id": "k1G2bzfP-nmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q Learning algorithm\n",
        "We are now going to implement the DQN algorithm. First, we need to encode the policy using a neural network. Create a neural network that takes a frame from the game, preprocesses it and returns a score for each possible action."
      ],
      "metadata": {
        "id": "vzy4BPxc8cJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    #\n",
        "    # YOUR CODE HERE\n",
        "    #"
      ],
      "metadata": {
        "id": "u0TeSzT68woI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define a replay memory to sample past (state, action, reward) tuples, and a utility function for plotting scores."
      ],
      "metadata": {
        "id": "fQ0CjnNh4BFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "def plot_scores(score_values, show_result=False):\n",
        "    plt.figure(1)\n",
        "    scores_t = torch.tensor(score_values, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.plot(scores_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(scores_t) >= 100:\n",
        "        means = scores_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if not show_result:\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "    else:\n",
        "        display.display(plt.gcf())"
      ],
      "metadata": {
        "id": "MDwtfE_68s1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimization step performs one step of Q-Learning. Fill in the blanks."
      ],
      "metadata": {
        "id": "kfhx7sHo7SP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = ### YOUR CODE HERE ###\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = ### YOUR CODE HERE ###\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "8FV2N3Nr8oAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the training loop!"
      ],
      "metadata": {
        "id": "QLKnqE8D9Cog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS = 0.1\n",
        "LR = 1e-4\n",
        "NUM_EPISODES = 100\n",
        "INIT_TRAINING = True\n",
        "\n",
        "if INIT_TRAINING:\n",
        "    policy_net = DQN().to(device)\n",
        "    epsilon_greedy = EpsilonGreedy(policy_net, EPS)\n",
        "\n",
        "    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "    memory = ReplayMemory(10000)\n",
        "    steps_done = 0\n",
        "    episode_durations = []\n",
        "    scores = []\n",
        "\n",
        "for i_episode in range(NUM_EPISODES):\n",
        "    # Initialize the environment and get its state\n",
        "    state = preprocess(env.reset())\n",
        "    score = 0\n",
        "    for t in count():\n",
        "        action = epsilon_greedy(state)\n",
        "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        observation = observation\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        done = terminated or truncated\n",
        "        score += reward.item()\n",
        "\n",
        "        next_state = None if terminated else preprocess(observation)\n",
        "        memory.push(state, action, next_state, reward)\n",
        "        state = next_state\n",
        "        optimize_model()\n",
        "\n",
        "        if done:\n",
        "            scores.append(score)\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_scores(scores)\n",
        "            break\n",
        "\n",
        "print('Complete')\n",
        "plot_scores(scores, show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IRdM8W8a8fSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a video of the new agent playing Breakout. How is the agent performing?"
      ],
      "metadata": {
        "id": "HkzV-N0471dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# YOUR CODE HERE\n",
        "#"
      ],
      "metadata": {
        "id": "a8mxv2MmDEv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}