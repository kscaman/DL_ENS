{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kscaman/DL_ENS/blob/main/TP/TP04_Autodiff_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJiauBGr6hUG"
      },
      "source": [
        "# TP04 - Automatic differentiation from scratch\n",
        "In this practical, our aim is to understand how Pytorch performs automatic differentiation by making **our very own pytorch library**! To do so, we will reimplement the `backward()` function of PyTorch using NumPy only. This method will be able to backpropagate the gradients through any computation graph made of simple functions and store the gradient of the loss for each input parameter."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from graphviz import Digraph\n",
        "from PIL import Image\n",
        "import IPython.display as display"
      ],
      "metadata": {
        "id": "c7xVzwgemkae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part A - Creating the computation graph"
      ],
      "metadata": {
        "id": "JyBvqMtjxuku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a `Tensor` class that contains three fields: a numpy array `data` containing the value of the tensor, a set `_parents` that contains the parent nodes (i.e. Tensors) in the computation graph, and a string `_function` that contains the name of the function used to compute it (if any). The `__init__` method should only take `data` as input, and set `_function` and `_parents` to default values (`None` and `set()`, respectively). Add a `__repr__` function to print the value of tensors in a proper format."
      ],
      "metadata": {
        "id": "bL8bMap2xXR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "Ql-Ol0mg0KS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write functions `add(tensor1, tensor2)` and `mul(tensor1, tensor2)` that perform addition and multiplication of tensors, and properly update the value of the output tensor's hidden fields (`_function` and `_parents`). We will assume, for simplicity, that tensors are only scalars at this point. Test your functions on simple examples."
      ],
      "metadata": {
        "id": "scRhwxSy4Sc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "f6GCiyX_4a6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now overload the operators `+` and `*` for tensors with your own functions. Test it on simple examples."
      ],
      "metadata": {
        "id": "wAYkRWr4-KA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "_8ggK3_s-Sy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the `trace_graph` function that returns a list of all nodes and edges (i.e. pairs of nodes) in the computation graph, and test the visualization tool with the function $x^2 + 4x + 2$."
      ],
      "metadata": {
        "id": "OqD24Z0K_Ars"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trace_graph(tensor):\n",
        "    \"\"\"\n",
        "    Trace the computational graph starting from a tensor.\n",
        "    Returns the nodes and edges of the graph.\n",
        "    \"\"\"\n",
        "    nodes, edges = set(), set()\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    return nodes, edges\n",
        "\n",
        "def draw_computation_graph(tensor):\n",
        "    \"\"\"\n",
        "    Plot the computation graph starting from a given tensor.\n",
        "    \"\"\"\n",
        "    dot = Digraph(format='png', graph_attr={'rankdir': 'LR'})  # Left-to-right layout\n",
        "\n",
        "    # Trace the graph and get nodes and edges\n",
        "    nodes, edges = trace_graph(tensor)\n",
        "\n",
        "    for node in nodes:\n",
        "        node_label = node._function if node._function else f\"{np.linalg.norm(node.data):.2f}\"\n",
        "        # Add node to the graph, label with its value\n",
        "        dot.node(str(id(node)), label=node_label, shape='circle')\n",
        "\n",
        "    for n1, n2 in edges:\n",
        "        # Create edges between nodes\n",
        "        dot.edge(str(id(n1)), str(id(n2)))\n",
        "\n",
        "    # Render and display the graph\n",
        "    graph_path = dot.render(filename='computation-graph')\n",
        "    img = Image.open(graph_path)\n",
        "    display.display(img)\n",
        "\n",
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "EogmdSoNnZ89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B - The backpropagation algorithm"
      ],
      "metadata": {
        "id": "8BdZDfoAyPmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement backpropagation by using tensors as nodes of the computation graph. To do so, each function used during the computation will add a `_backward` method to its output tensor. The purpose of this method is to update the gradient (`grad`) of the function's input tensors according to the chain rule (see the new `add` function below). Modify `mul` accordingly."
      ],
      "metadata": {
        "id": "ite-dFa8CDu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add(tensor1, tensor2):\n",
        "    out = Tensor(tensor1.data + tensor2.data)\n",
        "\n",
        "    def _backward():\n",
        "        tensor1.grad = tensor1.grad + out.grad\n",
        "        tensor2.grad = tensor2.grad + out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    out._function = \"add\"\n",
        "    out._parents = {tensor1, tensor2}\n",
        "\n",
        "    return out\n",
        "\n",
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "x3AS3DZHnRvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add another function of your choice (e.g. power, exp, log,...)."
      ],
      "metadata": {
        "id": "--7D4V_wEjX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "dEOYr2ikEoic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now extend the `Tensor` class to also contains the fields:\n",
        "\n",
        "1.   `grad` (default value: `None`) that will contain, after calling `backward()`, the gradient of the output with respect to this tensor.\n",
        "2.   `_backward` (default value: `lambda: None`) that will implement the chain rule for the function whose output is this tensor.\n",
        "\n",
        "Finally add a method `backward(self)` that implements the backpropagation algorithm. To do so:\n",
        "\n",
        "1. Build a list of tensors (i.e. nodes of the computation graph) that contains all nodes, from the current tensor (generally the final output or loss) to the roots (the input tensors or parameters in a learning setting), and such that parents appear after their children in the list.\n",
        "2. Initialize all nodes' gradients `node.grad` to zero.\n",
        "3. Initialize the current tensor's gradient `self.grad` to one.\n",
        "4. Apply the `node._backward()` function for each node in the list.\n",
        "\n",
        "To test your backpropagation algorithm, compute the gradient of $x*x + 5x - 1$ at $x=2$."
      ],
      "metadata": {
        "id": "gNTd-4ctAuDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "s57X5nrMnPF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part C - Optimization with gradient descent"
      ],
      "metadata": {
        "id": "_rUtPKOwyE62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have implemented automatic differentiation, use it to perform gradient descent on a function of your choice."
      ],
      "metadata": {
        "id": "Ne3_NUMvGl4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "Wy3GC2GHnXNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part D - Matrix and regression setting (optional)\n",
        "Update the code to allow for matrix-vector products and the ReLU function, and test it on a regression problem with a 2-layer MLP."
      ],
      "metadata": {
        "id": "1Kicj9v1lLGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "7kbDmiC-udLc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIUWAyBkqXrb594trnxX/B",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}