{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNE9TBAZf3LPgoA/sstmvfs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kscaman/DL_ENS/blob/main/TP/robustness_regularity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robustness and regularity\n",
        "In this practical, we will investigate the effect of initialization on simple neural networks (MLPs)."
      ],
      "metadata": {
        "id": "CrnrhOhxICBY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnktggQTIBiT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to automatically create large and deep MLPs. Create a function `MLP(dim_input, dim_output, dim_hidden, num_layers)` that returns an MLP with ReLU activations, `num_layers` layers and width `dim_hidden` using `nn.Sequential`."
      ],
      "metadata": {
        "id": "C7mKyWYAn6X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "8rA7GbucmSnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that the MLP has the correct architecture for 1, 2 and 4 layers."
      ],
      "metadata": {
        "id": "JCQ-NNoJ2woO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(MLP(3,5,10,1))\n",
        "print(MLP(3,5,10,2))\n",
        "print(MLP(3,5,10,4))"
      ],
      "metadata": {
        "id": "VvpRA9apnNnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stability during training\n",
        "We are now going to experiment with initialization. First, let's plot the function created by an MLP at initialization. The MLP will have 4 layers of width 100. Use `torch.linspace` to plot the output of the MLP on the interval $[-1,1]$."
      ],
      "metadata": {
        "id": "k6aWQ4Kx3EZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "qlilulhZ484F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot 10 functions on the same figure to have an idea of the variability of the initialization."
      ],
      "metadata": {
        "id": "lCU-tKVV7snm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "8m1q5N2y4MjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase the number of layers to 10. What happens? Is that a problem for learning?"
      ],
      "metadata": {
        "id": "kBKFLf5v72UN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "L4ecdddi8Acc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to fix this issue by applying a different initialization.\n",
        "Create a function that initializes all weights of the MLP by using functions in [`nn.init`](https://pytorch.org/docs/stable/nn.init.html). To do so, first create a function `init_weights(m)` that takes a layer, check that it is linear using `isinstance(m, nn.Linear)`, and, if so, re-initializes its weights. Then, use `model.apply` to apply the function `init_weights` to all the linear layers of the model.\n",
        "\n",
        "Again, plot 10 MLP outputs on the same figure. Is this initialization better?"
      ],
      "metadata": {
        "id": "hjqr-Pgd8Llb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "D4q1_2JFnVL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now look at the distribution of values for a single input (e.g. x=1).\n",
        "Plot a histogram of outputs for random initializations of the weights."
      ],
      "metadata": {
        "id": "3cgvnCIjBa-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "rTtEB5JuyILI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixing the initialization with batch normalization.\n",
        "Create a new function `MLP_bn(dim_input, dim_output, dim_hidden, num_layers)` that creates an MLP in which a batch norm `nn.BatchNorm1d` layer is added after each hidden layer."
      ],
      "metadata": {
        "id": "oUE4qBqB5xj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "AkW4mGPm5wDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How is the result different at initialization? Plot several functions generated by a 10-layer MLP at initialization (with default initialization)."
      ],
      "metadata": {
        "id": "Qksx_9L864FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "L0w9gDmo6hzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âš  **Careful though:** Batch norm depends on the **whole batch**, and uses the **training mean and standard deviation** during **evaluation**."
      ],
      "metadata": {
        "id": "4RU8WzYYDDrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH TRAINING DATASET ON [-1,1]\n",
        "model = MLP_bn(1, 1 , 100, 10)\n",
        "model.train()\n",
        "x = torch.linspace(-1, 1, 100).view(-1, 1)\n",
        "y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()\n",
        "\n",
        "model.eval()\n",
        "x = torch.linspace(-1e-3, 1e-3, 100).view(-1, 1)\n",
        "y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_e_FjLD8ThgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH TRAINING DATASET ON [-1e-3,1e-3]\n",
        "model = MLP_bn(1, 1 , 100, 10)\n",
        "model.train()\n",
        "x = torch.linspace(-1e-3, 1e-3, 100).view(-1, 1)\n",
        "for _ in range(1000):\n",
        "    y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()\n",
        "\n",
        "model.eval()\n",
        "x = torch.linspace(-1e-3, 1e-3, 100).view(-1, 1)\n",
        "y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N2-E6VybXmp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalization and overfitting\n",
        "We now investigate the generaliation capabilities of MLPs on a simple regression task $f(x)=\\sin(3x) + \\varepsilon$ where $\\varepsilon$ is a Gaussian noise of standard deviation $0.3$."
      ],
      "metadata": {
        "id": "9GUj2cJzhjM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "num_points = 50\n",
        "x_train = 4 * (2 * torch.rand(num_points, 1) - 1)\n",
        "y_train = torch.sin(3 * x_train) + 0.3 * torch.randn_like(x_train)\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "x_test = 4 * (2 * torch.rand(num_points, 1) - 1)\n",
        "y_test = torch.sin(3 * x_test) + 0.3 * torch.randn_like(x_test)\n",
        "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "x = torch.linspace(-4, 4, 1000)\n",
        "plt.plot(x_train, y_train, '.', label=\"train\")\n",
        "plt.plot(x, torch.sin(3 * x), label=\"target\")\n",
        "plt.legend()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('sin(3x)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LFWCOcwxuO3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a training pipeline for MLPs of width `d`."
      ],
      "metadata": {
        "id": "cYknu6SQo5O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(d):\n",
        "    model = MLP_bn(1, 1, d, 8).to(device)\n",
        "    loss_function = nn.MSELoss(reduction='sum')\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.6)\n",
        "    return model, loss_function, optimizer, scheduler\n",
        "\n",
        "def train(model, loss_function, optimizer):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for input, target in train_dataloader:\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        losses.append(loss.item())\n",
        "    return np.sum(losses) / len(train_dataloader)\n",
        "\n",
        "def test(model, loss_function):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for input, target in test_dataloader:\n",
        "            ### YOUR CODE HERE ###\n",
        "\n",
        "            losses.append(loss.item())\n",
        "    return np.sum(losses) / len(test_dataloader)\n",
        "\n",
        "def training_loop(d, num_epochs):\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    plt.loglog(train_losses, label=\"train\")\n",
        "    plt.loglog(test_losses, label=\"test\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Number of epochs\")\n",
        "    plt.ylabel(\"train and test losses\")\n",
        "    plt.show()\n",
        "    return train_losses[-1], test_losses[-1], model"
      ],
      "metadata": {
        "id": "NRQlD8QThvKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For increasing model sizes, the training error decreases. However, the test error first decreases then increases due to the model overfitting the data. If the number of parameters increases drastically, this tends to regularize/smoothen the model, and thus improve generalization. This is called **implicit regularization**."
      ],
      "metadata": {
        "id": "rRlyL_2NpF_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dim_hidden = [int(d) for d in np.unique(np.round(10 ** np.linspace(0, 2, 30)))]\n",
        "train_losses, test_losses = [], []\n",
        "for d in tqdm(dim_hidden):\n",
        "    train_loss, test_loss, model = training_loop(d, 2000)\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "plt.loglog(dim_hidden, train_losses, label=\"train\")\n",
        "plt.loglog(dim_hidden, test_losses, label=\"test\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"width of the MLP\")\n",
        "plt.ylabel(\"train and test losses\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FBWwIST7uTQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a large model ($d=10^2$) and plot the target function and the output of the model. Is it smoother?"
      ],
      "metadata": {
        "id": "arYGo3-zhzwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "ydf-H2vhGjYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a weight decay (`weight_decay=3`) to the optimizer. Is the output of the model smoother?"
      ],
      "metadata": {
        "id": "kUAoH7PclZze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "4gYWmYKBh-7q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}