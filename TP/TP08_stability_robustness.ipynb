{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOG1M61zhQh+bFPNf3uvXCb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kscaman/DL_ENS/blob/main/TP/TP08_stability_robustness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP08 - Stability and robustness\n",
        "In this practical, we are going to investigate the stability and robustness of neural networks, during training and at inference. To do so, we will test different initialization schemes, see their effect on the regularity of the function, test different methods to improve stability in the presence of outliers in the training distribution, and learn to generate adversarial attacks on pre-trained models."
      ],
      "metadata": {
        "id": "A3MoeiO7Jfww"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnktggQTIBiT"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A - Weight initialization\n",
        "Let's now investigate the effect of initialization on simple neural networks (MLPs).\n",
        "\n",
        "## A.0 - Model creation\n",
        "First, we need to automatically create large and deep MLPs. Create a function `MLP(dim_input, dim_output, dim_hidden, num_layers)` that returns an MLP with ReLU activations, `num_layers` layers and width `dim_hidden` using `nn.Sequential`.\n",
        "\n",
        "Check that the MLP has the correct architecture for 1, 2 and 4 layers."
      ],
      "metadata": {
        "id": "CrnrhOhxICBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "8rA7GbucmSnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.1 - Neural networks at initialization\n",
        "We are now going to experiment with initialization. First, let's plot the function created by an MLP at initialization."
      ],
      "metadata": {
        "id": "k6aWQ4Kx3EZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "qlilulhZ484F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot multiple functions on the same figure."
      ],
      "metadata": {
        "id": "lCU-tKVV7snm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "8m1q5N2y4MjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase the number of layers to 10. What happens? Is that a problem for learning?"
      ],
      "metadata": {
        "id": "kBKFLf5v72UN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "L4ecdddi8Acc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to fix this issue by applying a different initialization.\n",
        "Create a function that initializes all weights of the MLP by using functions in [`nn.init`](https://pytorch.org/docs/stable/nn.init.html)."
      ],
      "metadata": {
        "id": "hjqr-Pgd8Llb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "model = MLP(1, 1 , 100, 10)\n",
        "for _ in range(10):\n",
        "    model.apply(init_weights)\n",
        "    x = torch.linspace(-1, 1, 100).view(-1, 1)\n",
        "    y = model(x)\n",
        "\n",
        "    plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D4q1_2JFnVL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.2 - Fixing the issue with batch normalization\n",
        "Add a batch norm `nn.BatchNorm1d` layer after each hidden layer."
      ],
      "metadata": {
        "id": "oUE4qBqB5xj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "AkW4mGPm5wDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How is the result different at initialization? Plot several functions generated by a 10-layer MLP at initialization (with default initialization)."
      ],
      "metadata": {
        "id": "Qksx_9L864FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "L0w9gDmo6hzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âš  **Careful though:** Batch norm depends on the **whole batch**, and uses the **training mean and standard deviation** during **evaluation**."
      ],
      "metadata": {
        "id": "4RU8WzYYDDrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH TRAINING DATASET ON [-1,1]\n",
        "model = MLP_bn(1, 1 , 100, 10)\n",
        "model.train()\n",
        "x = torch.linspace(-1, 1, 100).view(-1, 1)\n",
        "y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()\n",
        "\n",
        "model.eval()\n",
        "x = torch.linspace(-1e-3, 1e-3, 100).view(-1, 1)\n",
        "y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_e_FjLD8ThgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH TRAINING DATASET ON [-1e-3,1e-3]\n",
        "model = MLP_bn(1, 1 , 100, 10)\n",
        "model.train()\n",
        "x = torch.linspace(-1e-3, 1e-3, 100).view(-1, 1)\n",
        "for _ in range(1000):\n",
        "    y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()\n",
        "\n",
        "model.eval()\n",
        "x = torch.linspace(-1e-3, 1e-3, 100).view(-1, 1)\n",
        "y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N2-E6VybXmp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B - Stability, outliers and overfitting\n",
        "We now investigate the generaliation capabilities of MLPs on a simple regression task: our aim is to lear the function $y=\\sin(3 x)$. However, a small number of training samples (denoted as **outliers**) were randomly perturbed by a large factor (of order 100)."
      ],
      "metadata": {
        "id": "9GUj2cJzhjM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "num_points = 200\n",
        "outlier_std = 100\n",
        "num_outliers = 5\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "x_train = 4 * (2 * torch.rand(num_points, 1) - 1)\n",
        "outlier_noise = torch.zeros_like(x_train)\n",
        "outlier_noise[torch.randperm(num_points)[:num_outliers]] = outlier_std * torch.randn(num_outliers, 1)\n",
        "y_train_clean = torch.sin(3 * x_train)\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train, y_train_clean, outlier_noise)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "x_test = 4 * (2 * torch.rand(100, 1) - 1)\n",
        "y_test = torch.sin(3 * x_test)\n",
        "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "def plot_function(model=None, zoom=True):\n",
        "    x = torch.linspace(-4, 4, 1000)\n",
        "    plt.plot(x_train, y_train_clean + outlier_noise, '.', label=\"train\")\n",
        "    plt.plot(x_test, y_test, '.', label=\"test\")\n",
        "    plt.plot(x, torch.sin(3 * x), label=\"target\")\n",
        "    if model is not None:\n",
        "        output = model(x.unsqueeze(1).to(device)).cpu().detach().numpy()\n",
        "        plt.plot(x, output, label=\"model\")\n",
        "    plt.legend()\n",
        "    if zoom:\n",
        "        plt.ylim([-2, 2])\n",
        "    plt.xlabel('input value ($x$)')\n",
        "    plt.ylabel('function value ($f(x)$)')\n",
        "    plt.show()\n",
        "\n",
        "plot_function(zoom=False)"
      ],
      "metadata": {
        "id": "LFWCOcwxuO3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create three functions:\n",
        "\n",
        "1.   A function `train_epoch(model, optimizer, clipping_threshold=None)` that trains the model for one epoch and applies gradient clipping if `clipping_threshold` is not `None`.\n",
        "2.   A function `test(model)` that returns the loss over the test set.\n",
        "3.   A function `train(model, optimizer, num_epochs, clipping_threshold=None)` that performs `num_epochs` epochs of training (with the MSE loss), and plots (in a figure) the train and test losses at each epoch.\n"
      ],
      "metadata": {
        "id": "cYknu6SQo5O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, optimizer, clipping_threshold):\n",
        "    model.train()\n",
        "    loss_function = nn.MSELoss(reduction='sum')\n",
        "    losses = []\n",
        "    for input, target, outlier_noise in train_dataloader:\n",
        "        input, target, outlier_noise = input.to(device), target.to(device), outlier_noise.to(device)\n",
        "        output = model(input)\n",
        "        target_with_outliers = target + outlier_noise\n",
        "        ### YOUR CODE HERE ###\n",
        "    return np.sum(losses) / len(train_dataloader)\n",
        "\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    loss_function = nn.MSELoss(reduction='sum')\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for input, target in test_dataloader:\n",
        "            ### YOUR CODE HERE ###\n",
        "    return np.sum(losses) / len(test_dataloader)\n",
        "\n",
        "def train(model, optimizer, num_epochs, clipping_threshold=None):\n",
        "    ### YOUR CODE HERE ###\n",
        "    pass"
      ],
      "metadata": {
        "id": "NRQlD8QThvKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train an MLP of width $10^3$ and depth $4$ with Adam (lr=1e-3), and plot the target function and the output of the model. The model tries to fit the outliers, lead to a poor performance on the test set."
      ],
      "metadata": {
        "id": "arYGo3-zhzwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "kDhfsM8Midqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to test 3 methods: 1) adding batch norm layers, 2) adding gradient clipping, and 3) adding regularization (aka weight decay). Test all three methods. Try to find reasonable parameters for clipping and weight decay by hand. Is the output of the model smoother? Can we reduce the impact of outliers?"
      ],
      "metadata": {
        "id": "1xSlTMznh-7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "KgGSfKOV23gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model with all three methods toghether. Is the test error better?"
      ],
      "metadata": {
        "id": "_PM5JJGv4cqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "pdtcmrp94lPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus question\n",
        "Use a validation set to find optimal parameters for the learning rate, weigth decay and gradient clipping."
      ],
      "metadata": {
        "id": "5-cJgLjI3Gmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "USiXmWTr3YbL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}